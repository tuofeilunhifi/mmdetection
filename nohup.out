/home/yunji.cjy/anaconda3/envs/openmmlab/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/home/yunji.cjy/codebase/mmdetection/mmdet/utils/setup_env.py:43: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  f'Setting MKL_NUM_THREADS environment variable for each process '
/home/yunji.cjy/codebase/mmdetection/mmdet/utils/setup_env.py:43: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  f'Setting MKL_NUM_THREADS environment variable for each process '
/home/yunji.cjy/codebase/mmdetection/mmdet/utils/setup_env.py:43: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  f'Setting MKL_NUM_THREADS environment variable for each process '
/home/yunji.cjy/codebase/mmdetection/mmdet/utils/setup_env.py:43: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  f'Setting MKL_NUM_THREADS environment variable for each process '
/home/yunji.cjy/codebase/mmdetection/mmdet/utils/setup_env.py:43: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  f'Setting MKL_NUM_THREADS environment variable for each process '
/home/yunji.cjy/codebase/mmdetection/mmdet/utils/setup_env.py:43: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  f'Setting MKL_NUM_THREADS environment variable for each process '
/home/yunji.cjy/codebase/mmdetection/mmdet/utils/setup_env.py:43: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  f'Setting MKL_NUM_THREADS environment variable for each process '
/home/yunji.cjy/codebase/mmdetection/mmdet/utils/setup_env.py:43: UserWarning: Setting MKL_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
  f'Setting MKL_NUM_THREADS environment variable for each process '
2022-04-15 11:33:25,382 - mmdet - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.7.13 (default, Mar 29 2022, 02:18:16) [GCC 7.5.0]
CUDA available: True
GPU 0,1,2,3,4,5,6,7: Tesla V100-SXM2-16GB
CUDA_HOME: /usr/local/cuda
NVCC: Cuda compilation tools, release 10.2, V10.2.89
GCC: gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-28)
PyTorch: 1.10.0
PyTorch compiling details: PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 10.2
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_37,code=compute_37
  - CuDNN 7.6.5
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.2, CUDNN_VERSION=7.6.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 

TorchVision: 0.11.0
OpenCV: 4.5.5
MMCV: 1.4.8
MMCV Compiler: GCC 7.3
MMCV CUDA Compiler: 10.2
MMDetection: 2.23.0+aaa0ce0
------------------------------------------------------------

2022-04-15 11:33:26,166 - mmdet - INFO - Distributed training: True
2022-04-15 11:33:26,944 - mmdet - INFO - Config:
norm_cfg = dict(type='SyncBN', requires_grad=True)
head_norm_cfg = dict(type='MMSyncBN', requires_grad=True)
model = dict(
    type='MaskRCNN',
    backbone=dict(
        type='ViTDetVisionTransformer',
        arch='b',
        img_size=1024,
        patch_size=16,
        window_size=16,
        drop_path_rate=0.1,
        out_indices=[11],
        final_norm=True,
        sincos_pos_embed=False,
        use_rel_pos_bias=True,
        init_cfg=dict(
            type='Pretrained',
            checkpoint=
            '/home/yunji.cjy/pretrain/warpper_mae_vit-base-p16-1600e.pth')),
    neck=dict(
        type='SFP',
        in_channels=768,
        out_channels=256,
        norm_cfg=dict(type='LN')),
    rpn_head=dict(
        type='RPNHead',
        in_channels=256,
        feat_channels=256,
        num_convs=2,
        anchor_generator=dict(
            type='AnchorGenerator',
            scales=[8],
            ratios=[0.5, 1.0, 2.0],
            strides=[4, 8, 16, 32]),
        bbox_coder=dict(
            type='DeltaXYWHBBoxCoder',
            target_means=[0.0, 0.0, 0.0, 0.0],
            target_stds=[1.0, 1.0, 1.0, 1.0]),
        loss_cls=dict(
            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),
        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
    roi_head=dict(
        type='StandardRoIHead',
        bbox_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        bbox_head=dict(
            type='Shared4Conv1FCBBoxHead',
            conv_out_channels=256,
            norm_cfg=dict(type='MMSyncBN', requires_grad=True),
            in_channels=256,
            fc_out_channels=1024,
            roi_feat_size=7,
            num_classes=80,
            bbox_coder=dict(
                type='DeltaXYWHBBoxCoder',
                target_means=[0.0, 0.0, 0.0, 0.0],
                target_stds=[0.1, 0.1, 0.2, 0.2]),
            reg_class_agnostic=False,
            loss_cls=dict(
                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),
            loss_bbox=dict(type='L1Loss', loss_weight=1.0)),
        mask_roi_extractor=dict(
            type='SingleRoIExtractor',
            roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=0),
            out_channels=256,
            featmap_strides=[4, 8, 16, 32]),
        mask_head=dict(
            type='FCNMaskHead',
            norm_cfg=dict(type='MMSyncBN', requires_grad=True),
            num_convs=4,
            in_channels=256,
            conv_out_channels=256,
            num_classes=80,
            loss_mask=dict(
                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))),
    train_cfg=dict(
        rpn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.7,
                neg_iou_thr=0.3,
                min_pos_iou=0.3,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=256,
                pos_fraction=0.5,
                neg_pos_ub=-1,
                add_gt_as_proposals=False),
            allowed_border=-1,
            pos_weight=-1,
            debug=False),
        rpn_proposal=dict(
            nms_pre=2000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            assigner=dict(
                type='MaxIoUAssigner',
                pos_iou_thr=0.5,
                neg_iou_thr=0.5,
                min_pos_iou=0.5,
                match_low_quality=True,
                ignore_iof_thr=-1),
            sampler=dict(
                type='RandomSampler',
                num=512,
                pos_fraction=0.25,
                neg_pos_ub=-1,
                add_gt_as_proposals=True),
            mask_size=28,
            pos_weight=-1,
            debug=False)),
    test_cfg=dict(
        rpn=dict(
            nms_pre=1000,
            max_per_img=1000,
            nms=dict(type='nms', iou_threshold=0.7),
            min_bbox_size=0),
        rcnn=dict(
            score_thr=0.05,
            nms=dict(type='nms', iou_threshold=0.5),
            max_per_img=100,
            mask_thr_binary=0.5)))
dataset_type = 'CocoDataset'
data_root = '/home/yunji.cjy/data/coco/'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
image_size = (1024, 1024)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
    dict(
        type='Resize',
        img_scale=(1024, 1024),
        ratio_range=(0.1, 2.0),
        multiscale_mode='range',
        keep_ratio=True),
    dict(
        type='RandomCrop',
        crop_type='absolute_range',
        crop_size=(1024, 1024),
        recompute_bbox=True,
        allow_negative_crop=True),
    dict(type='FilterAnnotations', min_gt_bbox_wh=(0.01, 0.01)),
    dict(type='RandomFlip', flip_ratio=0.5),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='Pad', size=(1024, 1024)),
    dict(type='DefaultFormatBundle'),
    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='MultiScaleFlipAug',
        img_scale=(1024, 1024),
        flip=False,
        transforms=[
            dict(type='Resize', keep_ratio=True),
            dict(type='RandomFlip'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='Pad', size=(1024, 1024)),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ])
]
data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type='RepeatDataset',
        times=4,
        dataset=dict(
            type='CocoDataset',
            ann_file=
            '/home/yunji.cjy/data/coco/annotations/instances_train2017.json',
            img_prefix='/home/yunji.cjy/data/coco/train2017/',
            pipeline=[
                dict(type='LoadImageFromFile'),
                dict(type='LoadAnnotations', with_bbox=True, with_mask=True),
                dict(
                    type='Resize',
                    img_scale=(1024, 1024),
                    ratio_range=(0.1, 2.0),
                    multiscale_mode='range',
                    keep_ratio=True),
                dict(
                    type='RandomCrop',
                    crop_type='absolute_range',
                    crop_size=(1024, 1024),
                    recompute_bbox=True,
                    allow_negative_crop=True),
                dict(type='FilterAnnotations', min_gt_bbox_wh=(0.01, 0.01)),
                dict(type='RandomFlip', flip_ratio=0.5),
                dict(
                    type='Normalize',
                    mean=[123.675, 116.28, 103.53],
                    std=[58.395, 57.12, 57.375],
                    to_rgb=True),
                dict(type='Pad', size=(1024, 1024)),
                dict(type='DefaultFormatBundle'),
                dict(
                    type='Collect',
                    keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])
            ])),
    val=dict(
        type='CocoDataset',
        ann_file='/home/yunji.cjy/data/coco/annotations/instances_val2017.json',
        img_prefix='/home/yunji.cjy/data/coco/val2017/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1024, 1024),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size=(1024, 1024)),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]),
    test=dict(
        type='CocoDataset',
        ann_file='/home/yunji.cjy/data/coco/annotations/instances_val2017.json',
        img_prefix='/home/yunji.cjy/data/coco/val2017/',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(
                type='MultiScaleFlipAug',
                img_scale=(1024, 1024),
                flip=False,
                transforms=[
                    dict(type='Resize', keep_ratio=True),
                    dict(type='RandomFlip'),
                    dict(
                        type='Normalize',
                        mean=[123.675, 116.28, 103.53],
                        std=[58.395, 57.12, 57.375],
                        to_rgb=True),
                    dict(type='Pad', size=(1024, 1024)),
                    dict(type='ImageToTensor', keys=['img']),
                    dict(type='Collect', keys=['img'])
                ])
        ]))
evaluation = dict(interval=1, metric=['bbox', 'segm'])
paramwise_cfg = dict(
    norm=dict(weight_decay=0.0),
    bias=dict(weight_decay=0.0),
    pos_embed=dict(weight_decay=0.0),
    cls_token=dict(weight_decay=0.0))
optimizer = dict(
    type='AdamW',
    lr=0.0001,
    betas=(0.9, 0.999),
    weight_decay=0.1,
    constructor='TransformerFinetuneConstructor',
    model_type='vit',
    layer_decay=0.7,
    paramwise_cfg=dict(
        norm=dict(weight_decay=0.0),
        bias=dict(weight_decay=0.0),
        pos_embed=dict(weight_decay=0.0),
        cls_token=dict(weight_decay=0.0)))
cumulative_iters = 4
optimizer_config = dict(grad_clip=None, cumulative_iters=4)
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=1000,
    warmup_ratio=0.001,
    step=[22, 24])
runner = dict(type='EpochBasedRunner', max_epochs=25)
fp16 = dict(loss_scale=dict(init_scale=512))
checkpoint_config = dict(interval=5)
log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])
custom_hooks = [dict(type='NumClassCheckHook')]
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = None
resume_from = None
workflow = [('train', 1)]
opencv_num_threads = 0
mp_start_method = 'fork'
find_unused_parameters = True
work_dir = './work_dirs/vitdet_sfp_100e_fp16_coco'
auto_resume = False
gpu_ids = range(0, 8)

2022-04-15 11:33:33,034 - mmdet - INFO - Set random seed to 486155617, deterministic: False
/home/yunji.cjy/anaconda3/envs/openmmlab/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272126608/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/yunji.cjy/anaconda3/envs/openmmlab/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272126608/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/yunji.cjy/anaconda3/envs/openmmlab/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272126608/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/yunji.cjy/anaconda3/envs/openmmlab/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272126608/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/yunji.cjy/anaconda3/envs/openmmlab/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272126608/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/yunji.cjy/anaconda3/envs/openmmlab/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272126608/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/yunji.cjy/anaconda3/envs/openmmlab/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272126608/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/yunji.cjy/anaconda3/envs/openmmlab/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272126608/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
2022-04-15 11:33:35,295 - mmdet - INFO - initialize ViTDetVisionTransformer with init_cfg {'type': 'Pretrained', 'checkpoint': '/home/yunji.cjy/pretrain/warpper_mae_vit-base-p16-1600e.pth'}
2022-04-15 11:33:35,296 - mmcv - INFO - load model from: /home/yunji.cjy/pretrain/warpper_mae_vit-base-p16-1600e.pth
2022-04-15 11:33:35,296 - mmcv - INFO - load checkpoint from local path: /home/yunji.cjy/pretrain/warpper_mae_vit-base-p16-1600e.pth
2022-04-15 11:33:35,568 - mmdet - INFO - Resize the pos_embed shape from torch.Size([1, 197, 768]) to torch.Size([1, 4097, 768]).
2022-04-15 11:33:35,753 - mmcv - WARNING - The model and loaded state dict do not match exactly

missing keys in source state_dict: window_rel_pos_bias.relative_position_bias_table, window_rel_pos_bias.relative_position_index, global_rel_pos_bias.relative_position_bias_table, global_rel_pos_bias.relative_position_index

2022-04-15 11:33:35,886 - mmdet - INFO - initialize SFP with init_cfg [{'type': 'Xavier', 'layer': ['Conv2d', 'ConvTranspose2d'], 'distribution': 'uniform'}, {'type': 'Constant', 'layer': ['GroupNorm'], 'val': 1}]
2022-04-15 11:33:35,976 - mmdet - INFO - initialize RPNHead with init_cfg {'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01}
2022-04-15 11:33:35,987 - mmdet - INFO - initialize Shared4Conv1FCBBoxHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}, {'type': 'Xavier', 'distribution': 'uniform', 'override': [{'name': 'shared_fcs'}, {'name': 'cls_fcs'}, {'name': 'reg_fcs'}]}]
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
2022-04-15 11:33:36,152 - mmdet - INFO - model:MaskRCNN(
  (backbone): ViTDetVisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
      (norm): Identity()
    )
    (window_rel_pos_bias): RelativePositionBias()
    (global_rel_pos_bias): RelativePositionBias()
    (drop_after_pos): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  )
  init_cfg={'type': 'Pretrained', 'checkpoint': '/home/yunji.cjy/pretrain/warpper_mae_vit-base-p16-1600e.pth'}
  (neck): SFP(
    (top_downs): ModuleList(
      (0): Sequential(
        (0): ConvTranspose2d(768, 768, kernel_size=(2, 2), stride=(2, 2))
        (1): GroupNorm(1, 768, eps=1e-06, affine=True)
        (2): GELU()
        (3): ConvTranspose2d(768, 768, kernel_size=(2, 2), stride=(2, 2))
      )
      (1): ConvTranspose2d(768, 768, kernel_size=(2, 2), stride=(2, 2))
      (2): Identity()
      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (sfp_outs): ModuleList(
      (0): Sequential(
        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(1, 256, eps=1e-06, affine=True)
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): GroupNorm(1, 256, eps=1e-06, affine=True)
      )
      (1): Sequential(
        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(1, 256, eps=1e-06, affine=True)
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): GroupNorm(1, 256, eps=1e-06, affine=True)
      )
      (2): Sequential(
        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(1, 256, eps=1e-06, affine=True)
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): GroupNorm(1, 256, eps=1e-06, affine=True)
      )
      (3): Sequential(
        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
        (1): GroupNorm(1, 256, eps=1e-06, affine=True)
        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): GroupNorm(1, 256, eps=1e-06, affine=True)
      )
    )
  )
  init_cfg=[{'type': 'Xavier', 'layer': ['Conv2d', 'ConvTranspose2d'], 'distribution': 'uniform'}, {'type': 'Constant', 'layer': ['GroupNorm'], 'val': 1}]
  (rpn_head): RPNHead(
    (loss_cls): CrossEntropyLoss(avg_non_ignore=False)
    (loss_bbox): L1Loss()
    (rpn_conv): Sequential(
      (0): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (activate): ReLU()
      )
      (1): ConvModule(
        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (activate): ReLU()
      )
    )
    (rpn_cls): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
    (rpn_reg): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
  )
  init_cfg={'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01}
  (roi_head): StandardRoIHead(
    (bbox_roi_extractor): SingleRoIExtractor(
      (roi_layers): ModuleList(
        (0): RoIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, pool_mode=avg, aligned=True, use_torchvision=False)
        (1): RoIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, pool_mode=avg, aligned=True, use_torchvision=False)
        (2): RoIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, pool_mode=avg, aligned=True, use_torchvision=False)
        (3): RoIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, pool_mode=avg, aligned=True, use_torchvision=False)
      )
    )
    (bbox_head): Shared4Conv1FCBBoxHead(
      (loss_cls): CrossEntropyLoss(avg_non_ignore=False)
      (loss_bbox): L1Loss()
      (fc_cls): Linear(in_features=1024, out_features=81, bias=True)
      (fc_reg): Linear(in_features=1024, out_features=320, bias=True)
      (shared_convs): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, group_size=8,stats_mode=default)
          (activate): ReLU(inplace=True)
        )
        (1): ConvModule(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, group_size=8,stats_mode=default)
          (activate): ReLU(inplace=True)
        )
        (2): ConvModule(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, group_size=8,stats_mode=default)
          (activate): ReLU(inplace=True)
        )
        (3): ConvModule(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, group_size=8,stats_mode=default)
          (activate): ReLU(inplace=True)
        )
      )
      (shared_fcs): ModuleList(
        (0): Linear(in_features=12544, out_features=1024, bias=True)
      )
      (cls_convs): ModuleList()
      (cls_fcs): ModuleList()
      (reg_convs): ModuleList()
      (reg_fcs): ModuleList()
      (relu): ReLU(inplace=True)
    )
    init_cfg=[{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}, {'type': 'Xavier', 'distribution': 'uniform', 'override': [{'name': 'shared_fcs'}, {'name': 'cls_fcs'}, {'name': 'reg_fcs'}]}]
    (mask_roi_extractor): SingleRoIExtractor(
      (roi_layers): ModuleList(
        (0): RoIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, pool_mode=avg, aligned=True, use_torchvision=False)
        (1): RoIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, pool_mode=avg, aligned=True, use_torchvision=False)
        (2): RoIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, pool_mode=avg, aligned=True, use_torchvision=False)
        (3): RoIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, pool_mode=avg, aligned=True, use_torchvision=False)
      )
    )
    (mask_head): FCNMaskHead(
      (loss_mask): CrossEntropyLoss(avg_non_ignore=False)
      (convs): ModuleList(
        (0): ConvModule(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, group_size=8,stats_mode=default)
          (activate): ReLU(inplace=True)
        )
        (1): ConvModule(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, group_size=8,stats_mode=default)
          (activate): ReLU(inplace=True)
        )
        (2): ConvModule(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, group_size=8,stats_mode=default)
          (activate): ReLU(inplace=True)
        )
        (3): ConvModule(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, group_size=8,stats_mode=default)
          (activate): ReLU(inplace=True)
        )
      )
      (upsample): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
      (conv_logits): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1))
      (relu): ReLU(inplace=True)
    )
  )
)
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
Done (t=18.54s)
creating index...
Done (t=18.54s)
creating index...
Done (t=18.55s)
creating index...
Done (t=18.62s)
creating index...
Done (t=18.64s)
creating index...
Done (t=18.65s)
creating index...
Done (t=18.63s)
creating index...
Done (t=18.63s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
paramwise_options --                                     backbone.cls_token: weight_decay=0.0
paramwise_options --                                     backbone.cls_token: lr=9.688901040699993e-07
paramwise_options --                                     backbone.pos_embed: weight_decay=0.0
paramwise_options --                                     backbone.pos_embed: lr=9.688901040699993e-07
paramwise_options --                                     backbone.patch_embed.proj.weight: lr=9.688901040699993e-07
paramwise_options --                                     backbone.patch_embed.proj.bias: weight_decay=0.0
paramwise_options --                                     backbone.patch_embed.proj.bias: lr=9.688901040699993e-07
paramwise_options --                                     backbone.window_rel_pos_bias.relative_position_bias_table: weight_decay=0.0
paramwise_options --                                     backbone.global_rel_pos_bias.relative_position_bias_table: weight_decay=0.0
paramwise_options --                                     backbone.blocks.0.norm1.weight: weight_decay=0.0
paramwise_options --                                     backbone.blocks.0.norm1.weight: lr=1.384128720099999e-06
paramwise_options --                                     backbone.blocks.0.norm1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.0.norm1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.0.norm1.bias: lr=1.384128720099999e-06
paramwise_options --                                     backbone.blocks.0.attn.qkv.weight: lr=1.384128720099999e-06
paramwise_options --                                     backbone.blocks.0.attn.qkv.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.0.attn.qkv.bias: lr=1.384128720099999e-06
paramwise_options --                                     backbone.blocks.0.attn.proj.weight: lr=1.384128720099999e-06
paramwise_options --                                     backbone.blocks.0.attn.proj.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.0.attn.proj.bias: lr=1.384128720099999e-06
paramwise_options --                                     backbone.blocks.0.norm2.weight: weight_decay=0.0
paramwise_options --                                     backbone.blocks.0.norm2.weight: lr=1.384128720099999e-06
paramwise_options --                                     backbone.blocks.0.norm2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.0.norm2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.0.norm2.bias: lr=1.384128720099999e-06
paramwise_options --                                     backbone.blocks.0.mlp.fc1.weight: lr=1.384128720099999e-06
paramwise_options --                                     backbone.blocks.0.mlp.fc1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.0.mlp.fc1.bias: lr=1.384128720099999e-06
paramwise_options --                                     backbone.blocks.0.mlp.fc2.weight: lr=1.384128720099999e-06
paramwise_options --                                     backbone.blocks.0.mlp.fc2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.0.mlp.fc2.bias: lr=1.384128720099999e-06
paramwise_options --                                     backbone.blocks.1.norm1.weight: weight_decay=0.0
paramwise_options --                                     backbone.blocks.1.norm1.weight: lr=1.977326742999999e-06
paramwise_options --                                     backbone.blocks.1.norm1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.1.norm1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.1.norm1.bias: lr=1.977326742999999e-06
paramwise_options --                                     backbone.blocks.1.attn.qkv.weight: lr=1.977326742999999e-06
paramwise_options --                                     backbone.blocks.1.attn.qkv.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.1.attn.qkv.bias: lr=1.977326742999999e-06
paramwise_options --                                     backbone.blocks.1.attn.proj.weight: lr=1.977326742999999e-06
paramwise_options --                                     backbone.blocks.1.attn.proj.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.1.attn.proj.bias: lr=1.977326742999999e-06
paramwise_options --                                     backbone.blocks.1.norm2.weight: weight_decay=0.0
paramwise_options --                                     backbone.blocks.1.norm2.weight: lr=1.977326742999999e-06
paramwise_options --                                     backbone.blocks.1.norm2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.1.norm2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.1.norm2.bias: lr=1.977326742999999e-06
paramwise_options --                                     backbone.blocks.1.mlp.fc1.weight: lr=1.977326742999999e-06
paramwise_options --                                     backbone.blocks.1.mlp.fc1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.1.mlp.fc1.bias: lr=1.977326742999999e-06
paramwise_options --                                     backbone.blocks.1.mlp.fc2.weight: lr=1.977326742999999e-06
paramwise_options --                                     backbone.blocks.1.mlp.fc2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.1.mlp.fc2.bias: lr=1.977326742999999e-06
paramwise_options --                                     backbone.blocks.2.norm1.weight: weight_decay=0.0
paramwise_options --                                     backbone.blocks.2.norm1.weight: lr=2.8247524899999986e-06
paramwise_options --                                     backbone.blocks.2.norm1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.2.norm1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.2.norm1.bias: lr=2.8247524899999986e-06
paramwise_options --                                     backbone.blocks.2.attn.qkv.weight: lr=2.8247524899999986e-06
paramwise_options --                                     backbone.blocks.2.attn.qkv.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.2.attn.qkv.bias: lr=2.8247524899999986e-06
paramwise_options --                                     backbone.blocks.2.attn.proj.weight: lr=2.8247524899999986e-06
paramwise_options --                                     backbone.blocks.2.attn.proj.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.2.attn.proj.bias: lr=2.8247524899999986e-06
paramwise_options --                                     backbone.blocks.2.norm2.weight: weight_decay=0.0
paramwise_options --                                     backbone.blocks.2.norm2.weight: lr=2.8247524899999986e-06
paramwise_options --                                     backbone.blocks.2.norm2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.2.norm2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.2.norm2.bias: lr=2.8247524899999986e-06
paramwise_options --                                     backbone.blocks.2.mlp.fc1.weight: lr=2.8247524899999986e-06
paramwise_options --                                     backbone.blocks.2.mlp.fc1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.2.mlp.fc1.bias: lr=2.8247524899999986e-06
paramwise_options --                                     backbone.blocks.2.mlp.fc2.weight: lr=2.8247524899999986e-06
paramwise_options --                                     backbone.blocks.2.mlp.fc2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.2.mlp.fc2.bias: lr=2.8247524899999986e-06
paramwise_options --                                     backbone.blocks.3.norm1.weight: weight_decay=0.0
paramwise_options --                                     backbone.blocks.3.norm1.weight: lr=4.035360699999998e-06
paramwise_options --                                     backbone.blocks.3.norm1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.3.norm1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.3.norm1.bias: lr=4.035360699999998e-06
paramwise_options --                                     backbone.blocks.3.attn.qkv.weight: lr=4.035360699999998e-06
paramwise_options --                                     backbone.blocks.3.attn.qkv.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.3.attn.qkv.bias: lr=4.035360699999998e-06
paramwise_options --                                     backbone.blocks.3.attn.proj.weight: lr=4.035360699999998e-06
paramwise_options --                                     backbone.blocks.3.attn.proj.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.3.attn.proj.bias: lr=4.035360699999998e-06
paramwise_options --                                     backbone.blocks.3.norm2.weight: weight_decay=0.0
paramwise_options --                                     backbone.blocks.3.norm2.weight: lr=4.035360699999998e-06
paramwise_options --                                     backbone.blocks.3.norm2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.3.norm2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.3.norm2.bias: lr=4.035360699999998e-06
paramwise_options --                                     backbone.blocks.3.mlp.fc1.weight: lr=4.035360699999998e-06
paramwise_options --                                     backbone.blocks.3.mlp.fc1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.3.mlp.fc1.bias: lr=4.035360699999998e-06
paramwise_options --                                     backbone.blocks.3.mlp.fc2.weight: lr=4.035360699999998e-06
paramwise_options --                                     backbone.blocks.3.mlp.fc2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.3.mlp.fc2.bias: lr=4.035360699999998e-06
paramwise_options --                                     backbone.blocks.4.norm1.weight: weight_decay=0.0
paramwise_options --                                     backbone.blocks.4.norm1.weight: lr=5.764800999999997e-06
paramwise_options --                                     backbone.blocks.4.norm1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.4.norm1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.4.norm1.bias: lr=5.764800999999997e-06
paramwise_options --                                     backbone.blocks.4.attn.qkv.weight: lr=5.764800999999997e-06
paramwise_options --                                     backbone.blocks.4.attn.qkv.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.4.attn.qkv.bias: lr=5.764800999999997e-06
paramwise_options --                                     backbone.blocks.4.attn.proj.weight: lr=5.764800999999997e-06
paramwise_options --                                     backbone.blocks.4.attn.proj.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.4.attn.proj.bias: lr=5.764800999999997e-06
paramwise_options --                                     backbone.blocks.4.norm2.weight: weight_decay=0.0
paramwise_options --                                     backbone.blocks.4.norm2.weight: lr=5.764800999999997e-06
paramwise_options --                                     backbone.blocks.4.norm2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.4.norm2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.4.norm2.bias: lr=5.764800999999997e-06
paramwise_options --                                     backbone.blocks.4.mlp.fc1.weight: lr=5.764800999999997e-06
paramwise_options --                                     backbone.blocks.4.mlp.fc1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.4.mlp.fc1.bias: lr=5.764800999999997e-06
paramwise_options --                                     backbone.blocks.4.mlp.fc2.weight: lr=5.764800999999997e-06
paramwise_options --                                     backbone.blocks.4.mlp.fc2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.4.mlp.fc2.bias: lr=5.764800999999997e-06
paramwise_options --                                     backbone.blocks.5.norm1.weight: weight_decay=0.0
paramwise_options --                                     backbone.blocks.5.norm1.weight: lr=8.235429999999996e-06
paramwise_options --                                     backbone.blocks.5.norm1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.5.norm1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.5.norm1.bias: lr=8.235429999999996e-06
paramwise_options --                                     backbone.blocks.5.attn.qkv.weight: lr=8.235429999999996e-06
paramwise_options --                                     backbone.blocks.5.attn.qkv.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.5.attn.qkv.bias: lr=8.235429999999996e-06
paramwise_options --                                     backbone.blocks.5.attn.proj.weight: lr=8.235429999999996e-06
paramwise_options --                                     backbone.blocks.5.attn.proj.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.5.attn.proj.bias: lr=8.235429999999996e-06
paramwise_options --                                     backbone.blocks.5.norm2.weight: weight_decay=0.0
paramwise_options --                                     backbone.blocks.5.norm2.weight: lr=8.235429999999996e-06
paramwise_options --                                     backbone.blocks.5.norm2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.5.norm2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.5.norm2.bias: lr=8.235429999999996e-06
paramwise_options --                                     backbone.blocks.5.mlp.fc1.weight: lr=8.235429999999996e-06
paramwise_options --                                     backbone.blocks.5.mlp.fc1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.5.mlp.fc1.bias: lr=8.235429999999996e-06
paramwise_options --                                     backbone.blocks.5.mlp.fc2.weight: lr=8.235429999999996e-06
paramwise_options --                                     backbone.blocks.5.mlp.fc2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.5.mlp.fc2.bias: lr=8.235429999999996e-06
paramwise_options --                                     backbone.blocks.6.norm1.weight: weight_decay=0.0
paramwise_options --                                     backbone.blocks.6.norm1.weight: lr=1.1764899999999996e-05
paramwise_options --                                     backbone.blocks.6.norm1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.6.norm1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.6.norm1.bias: lr=1.1764899999999996e-05
paramwise_options --                                     backbone.blocks.6.attn.qkv.weight: lr=1.1764899999999996e-05
paramwise_options --                                     backbone.blocks.6.attn.qkv.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.6.attn.qkv.bias: lr=1.1764899999999996e-05
paramwise_options --                                     backbone.blocks.6.attn.proj.weight: lr=1.1764899999999996e-05
paramwise_options --                                     backbone.blocks.6.attn.proj.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.6.attn.proj.bias: lr=1.1764899999999996e-05
paramwise_options --                                     backbone.blocks.6.norm2.weight: weight_decay=0.0
paramwise_options --                                     backbone.blocks.6.norm2.weight: lr=1.1764899999999996e-05
paramwise_options --                                     backbone.blocks.6.norm2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.6.norm2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.6.norm2.bias: lr=1.1764899999999996e-05
paramwise_options --                                     backbone.blocks.6.mlp.fc1.weight: lr=1.1764899999999996e-05
paramwise_options --                                     backbone.blocks.6.mlp.fc1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.6.mlp.fc1.bias: lr=1.1764899999999996e-05
paramwise_options --                                     backbone.blocks.6.mlp.fc2.weight: lr=1.1764899999999996e-05
paramwise_options --                                     backbone.blocks.6.mlp.fc2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.6.mlp.fc2.bias: lr=1.1764899999999996e-05
paramwise_options --                                     backbone.blocks.7.norm1.weight: weight_decay=0.0
paramwise_options --                                     backbone.blocks.7.norm1.weight: lr=1.6806999999999993e-05
paramwise_options --                                     backbone.blocks.7.norm1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.7.norm1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.7.norm1.bias: lr=1.6806999999999993e-05
paramwise_options --                                     backbone.blocks.7.attn.qkv.weight: lr=1.6806999999999993e-05
paramwise_options --                                     backbone.blocks.7.attn.qkv.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.7.attn.qkv.bias: lr=1.6806999999999993e-05
paramwise_options --                                     backbone.blocks.7.attn.proj.weight: lr=1.6806999999999993e-05
paramwise_options --                                     backbone.blocks.7.attn.proj.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.7.attn.proj.bias: lr=1.6806999999999993e-05
paramwise_options --                                     backbone.blocks.7.norm2.weight: weight_decay=0.0
paramwise_options --                                     backbone.blocks.7.norm2.weight: lr=1.6806999999999993e-05
paramwise_options --                                     backbone.blocks.7.norm2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.7.norm2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.7.norm2.bias: lr=1.6806999999999993e-05
paramwise_options --                                     backbone.blocks.7.mlp.fc1.weight: lr=1.6806999999999993e-05
paramwise_options --                                     backbone.blocks.7.mlp.fc1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.7.mlp.fc1.bias: lr=1.6806999999999993e-05
paramwise_options --                                     backbone.blocks.7.mlp.fc2.weight: lr=1.6806999999999993e-05
paramwise_options --                                     backbone.blocks.7.mlp.fc2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.7.mlp.fc2.bias: lr=1.6806999999999993e-05
paramwise_options --                                     backbone.blocks.8.norm1.weight: weight_decay=0.0
paramwise_options --                                     backbone.blocks.8.norm1.weight: lr=2.4009999999999995e-05
paramwise_options --                                     backbone.blocks.8.norm1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.8.norm1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.8.norm1.bias: lr=2.4009999999999995e-05
paramwise_options --                                     backbone.blocks.8.attn.qkv.weight: lr=2.4009999999999995e-05
paramwise_options --                                     backbone.blocks.8.attn.qkv.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.8.attn.qkv.bias: lr=2.4009999999999995e-05
paramwise_options --                                     backbone.blocks.8.attn.proj.weight: lr=2.4009999999999995e-05
paramwise_options --                                     backbone.blocks.8.attn.proj.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.8.attn.proj.bias: lr=2.4009999999999995e-05
paramwise_options --                                     backbone.blocks.8.norm2.weight: weight_decay=0.0
paramwise_options --                                     backbone.blocks.8.norm2.weight: lr=2.4009999999999995e-05
paramwise_options --                                     backbone.blocks.8.norm2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.8.norm2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.8.norm2.bias: lr=2.4009999999999995e-05
paramwise_options --                                     backbone.blocks.8.mlp.fc1.weight: lr=2.4009999999999995e-05
paramwise_options --                                     backbone.blocks.8.mlp.fc1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.8.mlp.fc1.bias: lr=2.4009999999999995e-05
paramwise_options --                                     backbone.blocks.8.mlp.fc2.weight: lr=2.4009999999999995e-05
paramwise_options --                                     backbone.blocks.8.mlp.fc2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.8.mlp.fc2.bias: lr=2.4009999999999995e-05
paramwise_options --                                     backbone.blocks.9.norm1.weight: weight_decay=0.0
paramwise_options --                                     backbone.blocks.9.norm1.weight: lr=3.4299999999999993e-05
paramwise_options --                                     backbone.blocks.9.norm1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.9.norm1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.9.norm1.bias: lr=3.4299999999999993e-05
paramwise_options --                                     backbone.blocks.9.attn.qkv.weight: lr=3.4299999999999993e-05
paramwise_options --                                     backbone.blocks.9.attn.qkv.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.9.attn.qkv.bias: lr=3.4299999999999993e-05
paramwise_options --                                     backbone.blocks.9.attn.proj.weight: lr=3.4299999999999993e-05
paramwise_options --                                     backbone.blocks.9.attn.proj.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.9.attn.proj.bias: lr=3.4299999999999993e-05
paramwise_options --                                     backbone.blocks.9.norm2.weight: weight_decay=0.0
paramwise_options --                                     backbone.blocks.9.norm2.weight: lr=3.4299999999999993e-05
paramwise_options --                                     backbone.blocks.9.norm2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.9.norm2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.9.norm2.bias: lr=3.4299999999999993e-05
paramwise_options --                                     backbone.blocks.9.mlp.fc1.weight: lr=3.4299999999999993e-05
paramwise_options --                                     backbone.blocks.9.mlp.fc1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.9.mlp.fc1.bias: lr=3.4299999999999993e-05
paramwise_options --                                     backbone.blocks.9.mlp.fc2.weight: lr=3.4299999999999993e-05
paramwise_options --                                     backbone.blocks.9.mlp.fc2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.9.mlp.fc2.bias: lr=3.4299999999999993e-05
paramwise_options --                                     backbone.blocks.10.norm1.weight: weight_decay=0.0
paramwise_options --                                     backbone.blocks.10.norm1.weight: lr=4.9e-05
paramwise_options --                                     backbone.blocks.10.norm1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.10.norm1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.10.norm1.bias: lr=4.9e-05
paramwise_options --                                     backbone.blocks.10.attn.qkv.weight: lr=4.9e-05
paramwise_options --                                     backbone.blocks.10.attn.qkv.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.10.attn.qkv.bias: lr=4.9e-05
paramwise_options --                                     backbone.blocks.10.attn.proj.weight: lr=4.9e-05
paramwise_options --                                     backbone.blocks.10.attn.proj.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.10.attn.proj.bias: lr=4.9e-05
paramwise_options --                                     backbone.blocks.10.norm2.weight: weight_decay=0.0
paramwise_options --                                     backbone.blocks.10.norm2.weight: lr=4.9e-05
paramwise_options --                                     backbone.blocks.10.norm2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.10.norm2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.10.norm2.bias: lr=4.9e-05
paramwise_options --                                     backbone.blocks.10.mlp.fc1.weight: lr=4.9e-05
paramwise_options --                                     backbone.blocks.10.mlp.fc1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.10.mlp.fc1.bias: lr=4.9e-05
paramwise_options --                                     backbone.blocks.10.mlp.fc2.weight: lr=4.9e-05
paramwise_options --                                     backbone.blocks.10.mlp.fc2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.10.mlp.fc2.bias: lr=4.9e-05
paramwise_options --                                     backbone.blocks.11.norm1.weight: weight_decay=0.0
paramwise_options --                                     backbone.blocks.11.norm1.weight: lr=7e-05
paramwise_options --                                     backbone.blocks.11.norm1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.11.norm1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.11.norm1.bias: lr=7e-05
paramwise_options --                                     backbone.blocks.11.attn.qkv.weight: lr=7e-05
paramwise_options --                                     backbone.blocks.11.attn.qkv.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.11.attn.qkv.bias: lr=7e-05
paramwise_options --                                     backbone.blocks.11.attn.proj.weight: lr=7e-05
paramwise_options --                                     backbone.blocks.11.attn.proj.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.11.attn.proj.bias: lr=7e-05
paramwise_options --                                     backbone.blocks.11.norm2.weight: weight_decay=0.0
paramwise_options --                                     backbone.blocks.11.norm2.weight: lr=7e-05
paramwise_options --                                     backbone.blocks.11.norm2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.11.norm2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.11.norm2.bias: lr=7e-05
paramwise_options --                                     backbone.blocks.11.mlp.fc1.weight: lr=7e-05
paramwise_options --                                     backbone.blocks.11.mlp.fc1.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.11.mlp.fc1.bias: lr=7e-05
paramwise_options --                                     backbone.blocks.11.mlp.fc2.weight: lr=7e-05
paramwise_options --                                     backbone.blocks.11.mlp.fc2.bias: weight_decay=0.0
paramwise_options --                                     backbone.blocks.11.mlp.fc2.bias: lr=7e-05
paramwise_options --                                     backbone.norm.weight: weight_decay=0.0
paramwise_options --                                     backbone.norm.bias: weight_decay=0.0
paramwise_options --                                     backbone.norm.bias: weight_decay=0.0
paramwise_options --                                     neck.top_downs.0.0.bias: weight_decay=0.0
paramwise_options --                                     neck.top_downs.0.1.bias: weight_decay=0.0
paramwise_options --                                     neck.top_downs.0.3.bias: weight_decay=0.0
paramwise_options --                                     neck.top_downs.1.bias: weight_decay=0.0
paramwise_options --                                     neck.sfp_outs.0.0.bias: weight_decay=0.0
paramwise_options --                                     neck.sfp_outs.0.1.bias: weight_decay=0.0
paramwise_options --                                     neck.sfp_outs.0.2.bias: weight_decay=0.0
paramwise_options --                                     neck.sfp_outs.0.3.bias: weight_decay=0.0
paramwise_options --                                     neck.sfp_outs.1.0.bias: weight_decay=0.0
paramwise_options --                                     neck.sfp_outs.1.1.bias: weight_decay=0.0
paramwise_options --                                     neck.sfp_outs.1.2.bias: weight_decay=0.0
paramwise_options --                                     neck.sfp_outs.1.3.bias: weight_decay=0.0
paramwise_options --                                     neck.sfp_outs.2.0.bias: weight_decay=0.0
paramwise_options --                                     neck.sfp_outs.2.1.bias: weight_decay=0.0
paramwise_options --                                     neck.sfp_outs.2.2.bias: weight_decay=0.0
paramwise_options --                                     neck.sfp_outs.2.3.bias: weight_decay=0.0
paramwise_options --                                     neck.sfp_outs.3.0.bias: weight_decay=0.0
paramwise_options --                                     neck.sfp_outs.3.1.bias: weight_decay=0.0
paramwise_options --                                     neck.sfp_outs.3.2.bias: weight_decay=0.0
paramwise_options --                                     neck.sfp_outs.3.3.bias: weight_decay=0.0
paramwise_options --                                     rpn_head.rpn_conv.0.conv.bias: weight_decay=0.0
paramwise_options --                                     rpn_head.rpn_conv.1.conv.bias: weight_decay=0.0
paramwise_options --                                     rpn_head.rpn_cls.bias: weight_decay=0.0
paramwise_options --                                     rpn_head.rpn_reg.bias: weight_decay=0.0
paramwise_options --                                     roi_head.bbox_head.fc_cls.bias: weight_decay=0.0
paramwise_options --                                     roi_head.bbox_head.fc_reg.bias: weight_decay=0.0
paramwise_options --                                     roi_head.bbox_head.shared_convs.0.bn.bias: weight_decay=0.0
paramwise_options --                                     roi_head.bbox_head.shared_convs.1.bn.bias: weight_decay=0.0
paramwise_options --                                     roi_head.bbox_head.shared_convs.2.bn.bias: weight_decay=0.0
paramwise_options --                                     roi_head.bbox_head.shared_convs.3.bn.bias: weight_decay=0.0
paramwise_options --                                     roi_head.bbox_head.shared_fcs.0.bias: weight_decay=0.0
paramwise_options --                                     roi_head.mask_head.convs.0.bn.bias: weight_decay=0.0
paramwise_options --                                     roi_head.mask_head.convs.1.bn.bias: weight_decay=0.0
paramwise_options --                                     roi_head.mask_head.convs.2.bn.bias: weight_decay=0.0
paramwise_options --                                     roi_head.mask_head.convs.3.bn.bias: weight_decay=0.0
paramwise_options --                                     roi_head.mask_head.upsample.bias: weight_decay=0.0
paramwise_options --                                     roi_head.mask_head.conv_logits.bias: weight_decay=0.0
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
Done (t=0.52s)
creating index...
Done (t=0.52s)
creating index...
Done (t=0.53s)
creating index...
Done (t=0.52s)
creating index...
Done (t=0.52s)
creating index...
Done (t=0.53s)
creating index...
Done (t=0.53s)
creating index...
Done (t=0.53s)
creating index...
index created!
index created!
index created!
index created!
index created!
index created!
index created!
index created!
2022-04-15 11:33:59,954 - mmdet - INFO - Start running, host: yunji.cjy@j38g08156.eu95sqa, work_dir: /home/yunji.cjy/codebase/mmdetection/work_dirs/vitdet_sfp_100e_fp16_coco
2022-04-15 11:33:59,954 - mmdet - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) StepLrUpdaterHook                  
(ABOVE_NORMAL) GradientCumulativeFp16OptimizerHook
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) StepLrUpdaterHook                  
(NORMAL      ) NumClassCheckHook                  
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) StepLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) GradientCumulativeFp16OptimizerHook
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) DistEvalHook                       
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(NORMAL      ) NumClassCheckHook                  
(NORMAL      ) DistSamplerSeedHook                
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2022-04-15 11:33:59,954 - mmdet - INFO - workflow: [('train', 1)], max: 25 epochs
2022-04-15 11:33:59,956 - mmdet - INFO - Checkpoints will be saved to /home/yunji.cjy/codebase/mmdetection/work_dirs/vitdet_sfp_100e_fp16_coco by HardDiskBackend.
2022-04-15 11:34:34,787 - mmdet - INFO - Epoch [1][50/29317]	lr: 4.840e-08, eta: 5 days, 21:47:34, time: 0.697, data_time: 0.238, memory: 10533, loss_rpn_cls: 0.6935, loss_rpn_bbox: 0.0919, loss_cls: 4.1790, acc: 12.2619, loss_bbox: 0.0293, loss_mask: 0.7728, loss: 5.7665
2022-04-15 11:34:58,468 - mmdet - INFO - Epoch [1][100/29317]	lr: 9.679e-08, eta: 4 days, 23:05:44, time: 0.474, data_time: 0.034, memory: 10718, loss_rpn_cls: 0.6847, loss_rpn_bbox: 0.0839, loss_cls: 2.7840, acc: 96.5606, loss_bbox: 0.0321, loss_mask: 0.7648, loss: 4.3494
2022-04-15 11:35:22,306 - mmdet - INFO - Epoch [1][150/29317]	lr: 1.452e-07, eta: 4 days, 15:44:23, time: 0.477, data_time: 0.040, memory: 10718, loss_rpn_cls: 0.6645, loss_rpn_bbox: 0.0909, loss_cls: 0.8136, acc: 97.5933, loss_bbox: 0.0489, loss_mask: 0.7505, loss: 2.3683
2022-04-15 11:35:46,544 - mmdet - INFO - Epoch [1][200/29317]	lr: 1.936e-07, eta: 4 days, 12:27:49, time: 0.485, data_time: 0.036, memory: 10729, loss_rpn_cls: 0.6086, loss_rpn_bbox: 0.0915, loss_cls: 0.2690, acc: 97.0962, loss_bbox: 0.0701, loss_mask: 0.7370, loss: 1.7761
2022-04-15 11:36:10,872 - mmdet - INFO - Epoch [1][250/29317]	lr: 2.420e-07, eta: 4 days, 10:34:12, time: 0.487, data_time: 0.036, memory: 10729, loss_rpn_cls: 0.4512, loss_rpn_bbox: 0.0912, loss_cls: 0.3068, acc: 96.7301, loss_bbox: 0.0871, loss_mask: 0.7189, loss: 1.6553
2022-04-15 11:36:35,099 - mmdet - INFO - Epoch [1][300/29317]	lr: 2.904e-07, eta: 4 days, 9:14:11, time: 0.485, data_time: 0.033, memory: 10903, loss_rpn_cls: 0.2689, loss_rpn_bbox: 0.0847, loss_cls: 0.3476, acc: 95.9938, loss_bbox: 0.1226, loss_mask: 0.7119, loss: 1.5357
2022-04-15 11:36:59,333 - mmdet - INFO - Epoch [1][350/29317]	lr: 3.388e-07, eta: 4 days, 8:17:09, time: 0.485, data_time: 0.044, memory: 10903, loss_rpn_cls: 0.2471, loss_rpn_bbox: 0.0880, loss_cls: 0.3473, acc: 95.4249, loss_bbox: 0.1434, loss_mask: 0.6960, loss: 1.5219
2022-04-15 11:37:23,346 - mmdet - INFO - Epoch [1][400/29317]	lr: 3.872e-07, eta: 4 days, 7:27:27, time: 0.480, data_time: 0.034, memory: 10903, loss_rpn_cls: 0.1999, loss_rpn_bbox: 0.0782, loss_cls: 0.3312, acc: 95.4924, loss_bbox: 0.1470, loss_mask: 0.6852, loss: 1.4415
2022-04-15 11:37:47,493 - mmdet - INFO - Epoch [1][450/29317]	lr: 4.356e-07, eta: 4 days, 6:52:29, time: 0.483, data_time: 0.042, memory: 10903, loss_rpn_cls: 0.1991, loss_rpn_bbox: 0.0829, loss_cls: 0.3500, acc: 95.0906, loss_bbox: 0.1616, loss_mask: 0.6693, loss: 1.4630
2022-04-15 11:38:11,348 - mmdet - INFO - Epoch [1][500/29317]	lr: 4.840e-07, eta: 4 days, 6:17:13, time: 0.477, data_time: 0.034, memory: 10903, loss_rpn_cls: 0.1743, loss_rpn_bbox: 0.0799, loss_cls: 0.3609, acc: 94.8943, loss_bbox: 0.1750, loss_mask: 0.6614, loss: 1.4515
2022-04-15 11:38:36,458 - mmdet - INFO - Epoch [1][550/29317]	lr: 5.324e-07, eta: 4 days, 6:16:12, time: 0.502, data_time: 0.036, memory: 10949, loss_rpn_cls: 0.1693, loss_rpn_bbox: 0.0863, loss_cls: 0.3750, acc: 94.5442, loss_bbox: 0.1835, loss_mask: 0.6494, loss: 1.4635
2022-04-15 11:39:00,496 - mmdet - INFO - Epoch [1][600/29317]	lr: 5.808e-07, eta: 4 days, 5:53:25, time: 0.481, data_time: 0.038, memory: 10954, loss_rpn_cls: 0.1513, loss_rpn_bbox: 0.0804, loss_cls: 0.3524, acc: 94.9199, loss_bbox: 0.1719, loss_mask: 0.6405, loss: 1.3965
2022-04-15 11:39:25,003 - mmdet - INFO - Epoch [1][650/29317]	lr: 6.291e-07, eta: 4 days, 5:42:55, time: 0.490, data_time: 0.042, memory: 11014, loss_rpn_cls: 0.1377, loss_rpn_bbox: 0.0839, loss_cls: 0.3695, acc: 94.5549, loss_bbox: 0.1841, loss_mask: 0.6276, loss: 1.4029
2022-04-15 11:39:48,956 - mmdet - INFO - Epoch [1][700/29317]	lr: 6.775e-07, eta: 4 days, 5:24:10, time: 0.479, data_time: 0.035, memory: 11014, loss_rpn_cls: 0.1402, loss_rpn_bbox: 0.0868, loss_cls: 0.3598, acc: 94.6880, loss_bbox: 0.1797, loss_mask: 0.6163, loss: 1.3827
2022-04-15 11:40:12,798 - mmdet - INFO - Epoch [1][750/29317]	lr: 7.259e-07, eta: 4 days, 5:06:07, time: 0.477, data_time: 0.039, memory: 11014, loss_rpn_cls: 0.1185, loss_rpn_bbox: 0.0779, loss_cls: 0.3661, acc: 94.5798, loss_bbox: 0.1872, loss_mask: 0.6065, loss: 1.3562
2022-04-15 11:40:36,942 - mmdet - INFO - Epoch [1][800/29317]	lr: 7.743e-07, eta: 4 days, 4:54:49, time: 0.483, data_time: 0.032, memory: 11014, loss_rpn_cls: 0.1248, loss_rpn_bbox: 0.0844, loss_cls: 0.3863, acc: 94.2925, loss_bbox: 0.1962, loss_mask: 0.6003, loss: 1.3920
2022-04-15 11:41:00,573 - mmdet - INFO - Epoch [1][850/29317]	lr: 8.227e-07, eta: 4 days, 4:37:30, time: 0.473, data_time: 0.043, memory: 11014, loss_rpn_cls: 0.1163, loss_rpn_bbox: 0.0787, loss_cls: 0.3725, acc: 94.4502, loss_bbox: 0.1908, loss_mask: 0.5886, loss: 1.3469
2022-04-15 11:41:24,134 - mmdet - INFO - Epoch [1][900/29317]	lr: 8.711e-07, eta: 4 days, 4:21:04, time: 0.471, data_time: 0.034, memory: 11014, loss_rpn_cls: 0.1148, loss_rpn_bbox: 0.0801, loss_cls: 0.3826, acc: 94.1729, loss_bbox: 0.2041, loss_mask: 0.5831, loss: 1.3646
2022-04-15 11:41:48,147 - mmdet - INFO - Epoch [1][950/29317]	lr: 9.195e-07, eta: 4 days, 4:12:09, time: 0.480, data_time: 0.037, memory: 11014, loss_rpn_cls: 0.1088, loss_rpn_bbox: 0.0786, loss_cls: 0.3699, acc: 94.3789, loss_bbox: 0.1937, loss_mask: 0.5810, loss: 1.3319
2022-04-15 11:42:12,033 - mmdet - INFO - Exp name: vitdet_sfp_100e_fp16_coco.py
2022-04-15 11:42:12,033 - mmdet - INFO - Epoch [1][1000/29317]	lr: 9.679e-07, eta: 4 days, 4:02:30, time: 0.478, data_time: 0.032, memory: 11014, loss_rpn_cls: 0.1008, loss_rpn_bbox: 0.0768, loss_cls: 0.3687, acc: 94.3928, loss_bbox: 0.1948, loss_mask: 0.5659, loss: 1.3070
2022-04-15 11:42:36,122 - mmdet - INFO - Epoch [1][1050/29317]	lr: 9.689e-07, eta: 4 days, 3:56:07, time: 0.482, data_time: 0.034, memory: 11081, loss_rpn_cls: 0.0949, loss_rpn_bbox: 0.0709, loss_cls: 0.3752, acc: 94.2806, loss_bbox: 0.2003, loss_mask: 0.5564, loss: 1.2977
2022-04-15 11:43:00,158 - mmdet - INFO - Epoch [1][1100/29317]	lr: 9.689e-07, eta: 4 days, 3:49:29, time: 0.480, data_time: 0.036, memory: 11081, loss_rpn_cls: 0.1054, loss_rpn_bbox: 0.0845, loss_cls: 0.3908, acc: 93.7522, loss_bbox: 0.2194, loss_mask: 0.5484, loss: 1.3484
2022-04-15 11:43:25,035 - mmdet - INFO - Epoch [1][1150/29317]	lr: 9.689e-07, eta: 4 days, 3:52:42, time: 0.498, data_time: 0.038, memory: 11081, loss_rpn_cls: 0.1007, loss_rpn_bbox: 0.0822, loss_cls: 0.4113, acc: 93.4751, loss_bbox: 0.2293, loss_mask: 0.5509, loss: 1.3743
2022-04-15 11:43:49,301 - mmdet - INFO - Epoch [1][1200/29317]	lr: 9.689e-07, eta: 4 days, 3:49:12, time: 0.485, data_time: 0.038, memory: 11081, loss_rpn_cls: 0.0980, loss_rpn_bbox: 0.0787, loss_cls: 0.4112, acc: 93.3857, loss_bbox: 0.2350, loss_mask: 0.5345, loss: 1.3575
2022-04-15 11:44:13,266 - mmdet - INFO - Epoch [1][1250/29317]	lr: 9.689e-07, eta: 4 days, 3:43:03, time: 0.479, data_time: 0.034, memory: 11085, loss_rpn_cls: 0.0923, loss_rpn_bbox: 0.0750, loss_cls: 0.3978, acc: 93.4341, loss_bbox: 0.2322, loss_mask: 0.5295, loss: 1.3269
2022-04-15 11:44:37,477 - mmdet - INFO - Epoch [1][1300/29317]	lr: 9.689e-07, eta: 4 days, 3:39:37, time: 0.484, data_time: 0.035, memory: 11085, loss_rpn_cls: 0.0869, loss_rpn_bbox: 0.0730, loss_cls: 0.4094, acc: 93.2061, loss_bbox: 0.2406, loss_mask: 0.5196, loss: 1.3294
2022-04-15 11:45:01,832 - mmdet - INFO - Epoch [1][1350/29317]	lr: 9.689e-07, eta: 4 days, 3:37:44, time: 0.487, data_time: 0.042, memory: 11085, loss_rpn_cls: 0.0942, loss_rpn_bbox: 0.0797, loss_cls: 0.4310, acc: 92.6035, loss_bbox: 0.2593, loss_mask: 0.5160, loss: 1.3802
2022-04-15 11:45:25,889 - mmdet - INFO - Epoch [1][1400/29317]	lr: 9.689e-07, eta: 4 days, 3:33:20, time: 0.481, data_time: 0.034, memory: 11090, loss_rpn_cls: 0.0879, loss_rpn_bbox: 0.0770, loss_cls: 0.4225, acc: 92.6418, loss_bbox: 0.2604, loss_mask: 0.5014, loss: 1.3491
2022-04-15 11:45:49,893 - mmdet - INFO - Epoch [1][1450/29317]	lr: 9.689e-07, eta: 4 days, 3:28:47, time: 0.480, data_time: 0.039, memory: 11090, loss_rpn_cls: 0.0829, loss_rpn_bbox: 0.0733, loss_cls: 0.4177, acc: 92.8691, loss_bbox: 0.2541, loss_mask: 0.5014, loss: 1.3294
2022-04-15 11:46:13,995 - mmdet - INFO - Epoch [1][1500/29317]	lr: 9.689e-07, eta: 4 days, 3:25:18, time: 0.482, data_time: 0.031, memory: 11090, loss_rpn_cls: 0.0799, loss_rpn_bbox: 0.0678, loss_cls: 0.4095, acc: 92.9165, loss_bbox: 0.2495, loss_mask: 0.4948, loss: 1.3015
2022-04-15 11:46:38,359 - mmdet - INFO - Epoch [1][1550/29317]	lr: 9.689e-07, eta: 4 days, 3:24:05, time: 0.487, data_time: 0.037, memory: 11090, loss_rpn_cls: 0.0795, loss_rpn_bbox: 0.0728, loss_cls: 0.4241, acc: 92.5606, loss_bbox: 0.2613, loss_mask: 0.4941, loss: 1.3318
2022-04-15 11:47:02,687 - mmdet - INFO - Epoch [1][1600/29317]	lr: 9.689e-07, eta: 4 days, 3:22:37, time: 0.487, data_time: 0.041, memory: 11090, loss_rpn_cls: 0.0770, loss_rpn_bbox: 0.0712, loss_cls: 0.4141, acc: 92.5747, loss_bbox: 0.2606, loss_mask: 0.4810, loss: 1.3040
2022-04-15 11:47:27,011 - mmdet - INFO - Epoch [1][1650/29317]	lr: 9.689e-07, eta: 4 days, 3:21:13, time: 0.487, data_time: 0.036, memory: 11090, loss_rpn_cls: 0.0778, loss_rpn_bbox: 0.0720, loss_cls: 0.4151, acc: 92.4050, loss_bbox: 0.2636, loss_mask: 0.4872, loss: 1.3157
2022-04-15 11:47:51,357 - mmdet - INFO - Epoch [1][1700/29317]	lr: 9.689e-07, eta: 4 days, 3:20:01, time: 0.487, data_time: 0.038, memory: 11090, loss_rpn_cls: 0.0785, loss_rpn_bbox: 0.0748, loss_cls: 0.4176, acc: 92.3320, loss_bbox: 0.2638, loss_mask: 0.4795, loss: 1.3143
2022-04-15 11:48:15,264 - mmdet - INFO - Epoch [1][1750/29317]	lr: 9.689e-07, eta: 4 days, 3:15:49, time: 0.478, data_time: 0.037, memory: 11090, loss_rpn_cls: 0.0769, loss_rpn_bbox: 0.0692, loss_cls: 0.4283, acc: 92.1846, loss_bbox: 0.2736, loss_mask: 0.4813, loss: 1.3294
2022-04-15 11:48:39,278 - mmdet - INFO - Epoch [1][1800/29317]	lr: 9.689e-07, eta: 4 days, 3:12:33, time: 0.480, data_time: 0.035, memory: 11090, loss_rpn_cls: 0.0770, loss_rpn_bbox: 0.0726, loss_cls: 0.4013, acc: 92.6655, loss_bbox: 0.2519, loss_mask: 0.4694, loss: 1.2722
2022-04-15 11:49:03,032 - mmdet - INFO - Epoch [1][1850/29317]	lr: 9.689e-07, eta: 4 days, 3:07:43, time: 0.475, data_time: 0.032, memory: 11090, loss_rpn_cls: 0.0735, loss_rpn_bbox: 0.0697, loss_cls: 0.4141, acc: 92.2839, loss_bbox: 0.2670, loss_mask: 0.4705, loss: 1.2948
2022-04-15 11:49:26,712 - mmdet - INFO - Epoch [1][1900/29317]	lr: 9.689e-07, eta: 4 days, 3:02:39, time: 0.474, data_time: 0.036, memory: 11090, loss_rpn_cls: 0.0768, loss_rpn_bbox: 0.0696, loss_cls: 0.4004, acc: 92.5303, loss_bbox: 0.2563, loss_mask: 0.4653, loss: 1.2683
2022-04-15 11:49:51,550 - mmdet - INFO - Epoch [1][1950/29317]	lr: 9.689e-07, eta: 4 days, 3:05:03, time: 0.497, data_time: 0.039, memory: 11094, loss_rpn_cls: 0.0787, loss_rpn_bbox: 0.0709, loss_cls: 0.4067, acc: 92.3506, loss_bbox: 0.2618, loss_mask: 0.4736, loss: 1.2918
2022-04-15 11:50:15,843 - mmdet - INFO - Exp name: vitdet_sfp_100e_fp16_coco.py
2022-04-15 11:50:15,844 - mmdet - INFO - Epoch [1][2000/29317]	lr: 9.689e-07, eta: 4 days, 3:03:59, time: 0.486, data_time: 0.035, memory: 11094, loss_rpn_cls: 0.0735, loss_rpn_bbox: 0.0674, loss_cls: 0.4040, acc: 92.2361, loss_bbox: 0.2659, loss_mask: 0.4535, loss: 1.2643
2022-04-15 11:50:40,103 - mmdet - INFO - Epoch [1][2050/29317]	lr: 9.689e-07, eta: 4 days, 3:02:46, time: 0.485, data_time: 0.036, memory: 11094, loss_rpn_cls: 0.0777, loss_rpn_bbox: 0.0719, loss_cls: 0.4127, acc: 91.9807, loss_bbox: 0.2770, loss_mask: 0.4555, loss: 1.2948
2022-04-15 11:51:03,868 - mmdet - INFO - Epoch [1][2100/29317]	lr: 9.689e-07, eta: 4 days, 2:58:42, time: 0.475, data_time: 0.035, memory: 11094, loss_rpn_cls: 0.0697, loss_rpn_bbox: 0.0643, loss_cls: 0.3922, acc: 92.4048, loss_bbox: 0.2585, loss_mask: 0.4427, loss: 1.2276
2022-04-15 11:51:28,146 - mmdet - INFO - Epoch [1][2150/29317]	lr: 9.689e-07, eta: 4 days, 2:57:45, time: 0.486, data_time: 0.042, memory: 11094, loss_rpn_cls: 0.0701, loss_rpn_bbox: 0.0683, loss_cls: 0.4063, acc: 91.9722, loss_bbox: 0.2767, loss_mask: 0.4411, loss: 1.2626
2022-04-15 11:51:52,293 - mmdet - INFO - Epoch [1][2200/29317]	lr: 9.689e-07, eta: 4 days, 2:55:58, time: 0.483, data_time: 0.040, memory: 11094, loss_rpn_cls: 0.0720, loss_rpn_bbox: 0.0643, loss_cls: 0.4021, acc: 92.0515, loss_bbox: 0.2706, loss_mask: 0.4352, loss: 1.2442
2022-04-15 11:52:16,960 - mmdet - INFO - Epoch [1][2250/29317]	lr: 9.689e-07, eta: 4 days, 2:57:16, time: 0.494, data_time: 0.035, memory: 11094, loss_rpn_cls: 0.0704, loss_rpn_bbox: 0.0679, loss_cls: 0.3921, acc: 92.2064, loss_bbox: 0.2645, loss_mask: 0.4433, loss: 1.2382
2022-04-15 11:52:41,027 - mmdet - INFO - Epoch [1][2300/29317]	lr: 9.689e-07, eta: 4 days, 2:55:12, time: 0.481, data_time: 0.031, memory: 11094, loss_rpn_cls: 0.0683, loss_rpn_bbox: 0.0662, loss_cls: 0.3944, acc: 91.9712, loss_bbox: 0.2716, loss_mask: 0.4401, loss: 1.2406
2022-04-15 11:53:05,624 - mmdet - INFO - Epoch [1][2350/29317]	lr: 9.689e-07, eta: 4 days, 2:55:58, time: 0.492, data_time: 0.044, memory: 11094, loss_rpn_cls: 0.0723, loss_rpn_bbox: 0.0684, loss_cls: 0.4007, acc: 91.7363, loss_bbox: 0.2787, loss_mask: 0.4416, loss: 1.2617
2022-04-15 11:53:29,817 - mmdet - INFO - Epoch [1][2400/29317]	lr: 9.689e-07, eta: 4 days, 2:54:38, time: 0.484, data_time: 0.038, memory: 11094, loss_rpn_cls: 0.0670, loss_rpn_bbox: 0.0668, loss_cls: 0.3871, acc: 91.9788, loss_bbox: 0.2739, loss_mask: 0.4376, loss: 1.2323
2022-04-15 11:53:54,501 - mmdet - INFO - Epoch [1][2450/29317]	lr: 9.689e-07, eta: 4 days, 2:55:47, time: 0.494, data_time: 0.037, memory: 11094, loss_rpn_cls: 0.0722, loss_rpn_bbox: 0.0679, loss_cls: 0.3816, acc: 91.9553, loss_bbox: 0.2737, loss_mask: 0.4272, loss: 1.2226
2022-04-15 11:54:18,692 - mmdet - INFO - Epoch [1][2500/29317]	lr: 9.689e-07, eta: 4 days, 2:54:28, time: 0.484, data_time: 0.033, memory: 11094, loss_rpn_cls: 0.0639, loss_rpn_bbox: 0.0638, loss_cls: 0.3803, acc: 92.1562, loss_bbox: 0.2643, loss_mask: 0.4324, loss: 1.2048
2022-04-15 11:54:42,848 - mmdet - INFO - Epoch [1][2550/29317]	lr: 9.689e-07, eta: 4 days, 2:53:01, time: 0.483, data_time: 0.034, memory: 11094, loss_rpn_cls: 0.0701, loss_rpn_bbox: 0.0678, loss_cls: 0.3890, acc: 91.8315, loss_bbox: 0.2686, loss_mask: 0.4293, loss: 1.2247
2022-04-15 11:55:07,352 - mmdet - INFO - Epoch [1][2600/29317]	lr: 9.689e-07, eta: 4 days, 2:53:14, time: 0.490, data_time: 0.040, memory: 11094, loss_rpn_cls: 0.0665, loss_rpn_bbox: 0.0655, loss_cls: 0.3918, acc: 91.5703, loss_bbox: 0.2770, loss_mask: 0.4227, loss: 1.2234
2022-04-15 11:55:31,003 - mmdet - INFO - Epoch [1][2650/29317]	lr: 9.689e-07, eta: 4 days, 2:49:31, time: 0.473, data_time: 0.038, memory: 11094, loss_rpn_cls: 0.0625, loss_rpn_bbox: 0.0628, loss_cls: 0.3801, acc: 91.9912, loss_bbox: 0.2681, loss_mask: 0.4216, loss: 1.1952
2022-04-15 11:55:55,601 - mmdet - INFO - Epoch [1][2700/29317]	lr: 9.689e-07, eta: 4 days, 2:50:08, time: 0.492, data_time: 0.031, memory: 11094, loss_rpn_cls: 0.0718, loss_rpn_bbox: 0.0709, loss_cls: 0.3992, acc: 91.3293, loss_bbox: 0.2879, loss_mask: 0.4214, loss: 1.2511
2022-04-15 11:56:19,582 - mmdet - INFO - Epoch [1][2750/29317]	lr: 9.689e-07, eta: 4 days, 2:48:05, time: 0.480, data_time: 0.036, memory: 11094, loss_rpn_cls: 0.0657, loss_rpn_bbox: 0.0652, loss_cls: 0.3810, acc: 91.9661, loss_bbox: 0.2685, loss_mask: 0.4195, loss: 1.2000
2022-04-15 11:56:43,271 - mmdet - INFO - Epoch [1][2800/29317]	lr: 9.689e-07, eta: 4 days, 2:44:47, time: 0.474, data_time: 0.030, memory: 11094, loss_rpn_cls: 0.0646, loss_rpn_bbox: 0.0620, loss_cls: 0.3660, acc: 92.1052, loss_bbox: 0.2654, loss_mask: 0.4179, loss: 1.1758
2022-04-15 11:57:07,777 - mmdet - INFO - Epoch [1][2850/29317]	lr: 9.689e-07, eta: 4 days, 2:45:04, time: 0.490, data_time: 0.037, memory: 11094, loss_rpn_cls: 0.0632, loss_rpn_bbox: 0.0651, loss_cls: 0.3829, acc: 91.5908, loss_bbox: 0.2761, loss_mask: 0.4231, loss: 1.2105
2022-04-15 11:57:31,995 - mmdet - INFO - Epoch [1][2900/29317]	lr: 9.689e-07, eta: 4 days, 2:44:06, time: 0.484, data_time: 0.033, memory: 11094, loss_rpn_cls: 0.0629, loss_rpn_bbox: 0.0599, loss_cls: 0.3667, acc: 92.1211, loss_bbox: 0.2598, loss_mask: 0.4183, loss: 1.1674
2022-04-15 11:57:56,283 - mmdet - INFO - Epoch [1][2950/29317]	lr: 9.689e-07, eta: 4 days, 2:43:28, time: 0.486, data_time: 0.035, memory: 11094, loss_rpn_cls: 0.0660, loss_rpn_bbox: 0.0664, loss_cls: 0.3777, acc: 91.8083, loss_bbox: 0.2712, loss_mask: 0.4035, loss: 1.1848
2022-04-15 11:58:20,299 - mmdet - INFO - Exp name: vitdet_sfp_100e_fp16_coco.py
2022-04-15 11:58:20,299 - mmdet - INFO - Epoch [1][3000/29317]	lr: 9.689e-07, eta: 4 days, 2:41:43, time: 0.480, data_time: 0.038, memory: 11094, loss_rpn_cls: 0.0681, loss_rpn_bbox: 0.0630, loss_cls: 0.3628, acc: 92.0193, loss_bbox: 0.2681, loss_mask: 0.4164, loss: 1.1785
2022-04-15 11:58:45,001 - mmdet - INFO - Epoch [1][3050/29317]	lr: 9.689e-07, eta: 4 days, 2:42:46, time: 0.494, data_time: 0.039, memory: 11094, loss_rpn_cls: 0.0632, loss_rpn_bbox: 0.0662, loss_cls: 0.3692, acc: 91.7227, loss_bbox: 0.2731, loss_mask: 0.4097, loss: 1.1815
2022-04-15 11:59:09,340 - mmdet - INFO - Epoch [1][3100/29317]	lr: 9.689e-07, eta: 4 days, 2:42:17, time: 0.486, data_time: 0.035, memory: 11094, loss_rpn_cls: 0.0611, loss_rpn_bbox: 0.0643, loss_cls: 0.3852, acc: 91.3105, loss_bbox: 0.2867, loss_mask: 0.4110, loss: 1.2082
2022-04-15 11:59:33,135 - mmdet - INFO - Epoch [1][3150/29317]	lr: 9.689e-07, eta: 4 days, 2:39:49, time: 0.476, data_time: 0.037, memory: 11094, loss_rpn_cls: 0.0578, loss_rpn_bbox: 0.0619, loss_cls: 0.3593, acc: 91.9878, loss_bbox: 0.2669, loss_mask: 0.4018, loss: 1.1477
2022-04-15 11:59:57,538 - mmdet - INFO - Epoch [1][3200/29317]	lr: 9.689e-07, eta: 4 days, 2:39:40, time: 0.488, data_time: 0.035, memory: 11094, loss_rpn_cls: 0.0640, loss_rpn_bbox: 0.0693, loss_cls: 0.3690, acc: 91.6086, loss_bbox: 0.2774, loss_mask: 0.4039, loss: 1.1836
2022-04-15 12:00:21,619 - mmdet - INFO - Epoch [1][3250/29317]	lr: 9.689e-07, eta: 4 days, 2:38:18, time: 0.482, data_time: 0.034, memory: 11094, loss_rpn_cls: 0.0623, loss_rpn_bbox: 0.0617, loss_cls: 0.3615, acc: 91.8943, loss_bbox: 0.2651, loss_mask: 0.4016, loss: 1.1522
2022-04-15 12:00:45,707 - mmdet - INFO - Epoch [1][3300/29317]	lr: 9.689e-07, eta: 4 days, 2:36:59, time: 0.482, data_time: 0.034, memory: 11094, loss_rpn_cls: 0.0633, loss_rpn_bbox: 0.0632, loss_cls: 0.3562, acc: 92.0652, loss_bbox: 0.2601, loss_mask: 0.4015, loss: 1.1443
2022-04-15 12:01:10,008 - mmdet - INFO - Epoch [1][3350/29317]	lr: 9.689e-07, eta: 4 days, 2:36:29, time: 0.486, data_time: 0.039, memory: 11094, loss_rpn_cls: 0.0604, loss_rpn_bbox: 0.0619, loss_cls: 0.3519, acc: 92.1238, loss_bbox: 0.2596, loss_mask: 0.3940, loss: 1.1278
2022-04-15 12:01:33,977 - mmdet - INFO - Epoch [1][3400/29317]	lr: 9.689e-07, eta: 4 days, 2:34:47, time: 0.479, data_time: 0.034, memory: 11094, loss_rpn_cls: 0.0580, loss_rpn_bbox: 0.0585, loss_cls: 0.3434, acc: 92.1223, loss_bbox: 0.2566, loss_mask: 0.3938, loss: 1.1103
2022-04-15 12:01:58,911 - mmdet - INFO - Epoch [1][3450/29317]	lr: 9.689e-07, eta: 4 days, 2:36:32, time: 0.499, data_time: 0.034, memory: 11094, loss_rpn_cls: 0.0579, loss_rpn_bbox: 0.0601, loss_cls: 0.3779, acc: 91.3289, loss_bbox: 0.2794, loss_mask: 0.3975, loss: 1.1728
2022-04-15 12:02:23,234 - mmdet - INFO - Epoch [1][3500/29317]	lr: 9.689e-07, eta: 4 days, 2:36:05, time: 0.486, data_time: 0.037, memory: 11094, loss_rpn_cls: 0.0606, loss_rpn_bbox: 0.0623, loss_cls: 0.3630, acc: 91.6775, loss_bbox: 0.2723, loss_mask: 0.3977, loss: 1.1559
2022-04-15 12:02:46,710 - mmdet - INFO - Epoch [1][3550/29317]	lr: 9.689e-07, eta: 4 days, 2:32:45, time: 0.470, data_time: 0.036, memory: 11094, loss_rpn_cls: 0.0542, loss_rpn_bbox: 0.0585, loss_cls: 0.3564, acc: 91.7632, loss_bbox: 0.2691, loss_mask: 0.3867, loss: 1.1250
2022-04-15 12:03:11,277 - mmdet - INFO - Epoch [1][3600/29317]	lr: 9.689e-07, eta: 4 days, 2:33:11, time: 0.491, data_time: 0.032, memory: 11094, loss_rpn_cls: 0.0630, loss_rpn_bbox: 0.0638, loss_cls: 0.3429, acc: 92.0475, loss_bbox: 0.2622, loss_mask: 0.3981, loss: 1.1299
2022-04-15 12:03:36,109 - mmdet - INFO - Epoch [1][3650/29317]	lr: 9.689e-07, eta: 4 days, 2:34:28, time: 0.497, data_time: 0.039, memory: 11094, loss_rpn_cls: 0.0619, loss_rpn_bbox: 0.0632, loss_cls: 0.3420, acc: 91.9456, loss_bbox: 0.2639, loss_mask: 0.3873, loss: 1.1183
2022-04-15 12:04:00,809 - mmdet - INFO - Epoch [1][3700/29317]	lr: 9.689e-07, eta: 4 days, 2:35:17, time: 0.494, data_time: 0.036, memory: 11094, loss_rpn_cls: 0.0581, loss_rpn_bbox: 0.0600, loss_cls: 0.3599, acc: 91.6147, loss_bbox: 0.2697, loss_mask: 0.3958, loss: 1.1434
2022-04-15 12:04:25,629 - mmdet - INFO - Epoch [1][3750/29317]	lr: 9.689e-07, eta: 4 days, 2:36:27, time: 0.496, data_time: 0.041, memory: 11094, loss_rpn_cls: 0.0633, loss_rpn_bbox: 0.0661, loss_cls: 0.3568, acc: 91.5486, loss_bbox: 0.2758, loss_mask: 0.3897, loss: 1.1517
2022-04-15 12:04:49,557 - mmdet - INFO - Epoch [1][3800/29317]	lr: 9.689e-07, eta: 4 days, 2:34:42, time: 0.478, data_time: 0.031, memory: 11094, loss_rpn_cls: 0.0587, loss_rpn_bbox: 0.0583, loss_cls: 0.3538, acc: 91.8787, loss_bbox: 0.2615, loss_mask: 0.3828, loss: 1.1150
2022-04-15 12:05:14,601 - mmdet - INFO - Epoch [1][3850/29317]	lr: 9.689e-07, eta: 4 days, 2:36:32, time: 0.501, data_time: 0.036, memory: 11094, loss_rpn_cls: 0.0649, loss_rpn_bbox: 0.0611, loss_cls: 0.3384, acc: 92.0857, loss_bbox: 0.2560, loss_mask: 0.3856, loss: 1.1060
2022-04-15 12:05:39,129 - mmdet - INFO - Epoch [1][3900/29317]	lr: 9.689e-07, eta: 4 days, 2:36:30, time: 0.489, data_time: 0.035, memory: 11094, loss_rpn_cls: 0.0587, loss_rpn_bbox: 0.0609, loss_cls: 0.3450, acc: 91.7568, loss_bbox: 0.2654, loss_mask: 0.3994, loss: 1.1293
2022-04-15 12:06:03,864 - mmdet - INFO - Epoch [1][3950/29317]	lr: 9.689e-07, eta: 4 days, 2:37:29, time: 0.496, data_time: 0.041, memory: 11094, loss_rpn_cls: 0.0622, loss_rpn_bbox: 0.0668, loss_cls: 0.3552, acc: 91.4890, loss_bbox: 0.2758, loss_mask: 0.3884, loss: 1.1483
2022-04-15 12:06:28,899 - mmdet - INFO - Exp name: vitdet_sfp_100e_fp16_coco.py
2022-04-15 12:06:28,899 - mmdet - INFO - Epoch [1][4000/29317]	lr: 9.689e-07, eta: 4 days, 2:39:08, time: 0.501, data_time: 0.035, memory: 11094, loss_rpn_cls: 0.0612, loss_rpn_bbox: 0.0620, loss_cls: 0.3361, acc: 91.9656, loss_bbox: 0.2600, loss_mask: 0.3854, loss: 1.1046
2022-04-15 12:06:52,979 - mmdet - INFO - Epoch [1][4050/29317]	lr: 9.689e-07, eta: 4 days, 2:37:54, time: 0.482, data_time: 0.036, memory: 11094, loss_rpn_cls: 0.0555, loss_rpn_bbox: 0.0576, loss_cls: 0.3368, acc: 92.0706, loss_bbox: 0.2553, loss_mask: 0.3956, loss: 1.1007
2022-04-15 12:07:16,899 - mmdet - INFO - Epoch [1][4100/29317]	lr: 9.689e-07, eta: 4 days, 2:36:09, time: 0.478, data_time: 0.033, memory: 11094, loss_rpn_cls: 0.0634, loss_rpn_bbox: 0.0627, loss_cls: 0.3427, acc: 91.9998, loss_bbox: 0.2564, loss_mask: 0.3910, loss: 1.1163
